{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0146': 226, '0251': 108, '3671': 256, '4613': 3550, '4849': 2876, '5946': 2065, '6377': 3599, '6573': 650, '7720': 1223}\n"
     ]
    }
   ],
   "source": [
    "#데이터 시각화 보여주기(가장 많이 떠든 사람)\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Kkma\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from soynlp.normalizer import *\n",
    "text=\"\"\n",
    "with open(r\"C:\\Users\\Msi\\Desktop\\jongmanhoney.txt\",encoding=\"utf-8\")as f:\n",
    "    text=f.read()\n",
    "r=re.compile(\"[#]\")\n",
    "list_data = re.findall('\\d+',text)\n",
    "list_data=[w for w in list_data if len(w)>3 and len(w)<5 and w!='2021'] #강제로 처리해주기\n",
    "list_data=sorted(list_data)\n",
    "bow={}\n",
    "for w in list_data:\n",
    "    if w not in bow.keys():\n",
    "        bow[w]=0\n",
    "    else:\n",
    "        bow[w]+=1\n",
    "        \n",
    "bow={w:bow[w] for w in bow.keys() if bow[w]>100}\n",
    "print(bow)\n",
    "\n",
    "from matplotlib import font_manager\n",
    "\n",
    "font_fname = 'C:\\\\Users\\\\Msi\\\\Desktop\\\\BlackHanSans-Regular.ttf'\n",
    "\n",
    "font_family = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "\n",
    "plt.rcParams[\"font.family\"] = font_family\n",
    "wc=WordCloud(font_fname,width=1200,height=800,scale=2.0,max_font_size=250)\n",
    "gen=wc.generate_from_frequencies(bow)\n",
    "plt.figure()\n",
    "plt.imshow(gen,interpolation='bilinear')\n",
    "wc.to_file(\"발표.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "#사전 훈련된 Word2Vec 임베딩(Pre-trained Word2Vec embedding)\n",
    "#사전에 학습시켜놓은 임베딩벡터들을 가지고 와서 모델의 입력으로 사용하는 것이 더 좋은 성능\n",
    "\n",
    "#영어\n",
    "import gensim\n",
    "model=gensim.models.KeyedVectors.load_word2vec_format(r'C:\\Users\\Msi\\Desktop\\GoogleNews-vectors-negative300.bin.gz',binary=True)\n",
    "print(model.vectors.shape)\n",
    "#모델의 크기는 3000000 x 300 3000000만개의 단어와 각 단어의 차원은 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40797037\n",
      "0.11652229\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('this','is'))#두 단어의 유사도 계산하기\n",
    "print(model.similarity('marvel','avengers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('광무', 0.5746680498123169), ('건양', 0.5569872856140137), ('중평', 0.5530575513839722), ('융희', 0.5521409511566162), ('초평', 0.5479120016098022), ('충혜왕', 0.5460560321807861), ('효공왕', 0.5406419038772583), ('무덕', 0.5385192632675171), ('백낙', 0.5381578803062439), ('충정왕', 0.5341129302978516)]\n"
     ]
    }
   ],
   "source": [
    "#한국어 Word2Vec\n",
    "import gensim\n",
    "model=gensim.models.Word2Vec.load(r'C:\\Users\\Msi\\Desktop\\ko.bin')\n",
    "result=model.wv.most_similar(\"광해\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 :  11314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20뉴스그룸 데이터 전처리하기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents=dataset.data\n",
    "print(\"총 샘플 수 : \",len(documents))\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df=pd.DataFrame({\"document\":documents})\n",
    "news_df['clean_doc']=news_df['document'].str.replace(\"[^a-zA-Z]\",\" \")\n",
    "news_df['clean_doc']=news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "news_df['clean_doc']=news_df['clean_doc'].apply(lambda x: x.lower())#소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\",float(\"NaN\"),inplace=True)\n",
    "news_df.isnull().values.any() #Null값이 있음을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 :  10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('총 샘플 수 : ',len(news_df)) #샘플수를 줄였다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')#불용어\n",
    "tokenized_doc=news_df['clean_doc'].apply(lambda x : x.split())\n",
    "tokenized_doc=tokenized_doc.apply(lambda x : [item for item in x if item not in stop_words])\n",
    "tokenized_doc=tokenized_doc.to_list()\n",
    "#불용어를 제거해서 단어의 수를 줄였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플수 :  10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#단어 1개 이하인 샘플의 인덱스를 찾아서 저장하고, 해당 샘플들은 제거.\n",
    "drop_train=[index for index, sentence in enumerate(tokenized_doc) if len(sentence)<=1]\n",
    "tokenized_doc=np.delete(tokenized_doc,drop_train,axis=0)\n",
    "print('총 샘플수 : ',len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx=tokenizer.word_index\n",
    "idx2word={v:k for k,v in word2idx.items()}\n",
    "encoded=tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기:  64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(word2idx)+1\n",
    "print(\"단어 집합의 크기: \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(government (51),annoying (3107))->0\n",
      "(received (634),interlaced (6718))->0\n",
      "(shame (4988),received (634))->1\n",
      "(world (70),occured (4294))->1\n",
      "(subsidizing (15228),babes (11210))->0\n"
     ]
    }
   ],
   "source": [
    "#네거티브 샘플링을 통한 데이터셋 구성하기\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "#네거티브 샘플링\n",
    "skip_grams=[skipgrams(sample,vocabulary_size=vocab_size,window_size=10) for sample in encoded[:10]]\n",
    "pairs,labels=skip_grams[0][0],skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}),{:s} ({:d}))->{:d}\".format(idx2word[pairs[i][0]],pairs[i][0],\n",
    "                                                  idx2word[pairs[i][1]],pairs[i][1],\n",
    "                                                  labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 1, 100)       6427700     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 100)       6427700     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_5[0][0]                \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "#Skip-Gram with Negative Sampling(SGNS) 구현하기\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding,Reshape,Activation,Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "\n",
    "#임베딩 차원은 100 사용자가 정하는 하이퍼파라미터다.\n",
    "embed_size=100\n",
    "w_inputs=Input(shape=(1,),dtype='int32')\n",
    "word_embedding=Embedding(vocab_size,embed_size)(w_inputs)\n",
    "\n",
    "#주변 단어를 위한 임베딩 테이블\n",
    "c_inputs=Input(shape=(1, ),dtype='int32')\n",
    "context_embedding=Embedding(vocab_size,embed_size)(c_inputs)\n",
    "\n",
    "dot_product=Dot(axes=2)([word_embedding,context_embedding])\n",
    "dot_product=Reshape((1,),input_shape=(1,1))(dot_product)\n",
    "output=Activation('sigmoid')(dot_product)\n",
    "model=Model(inputs=[w_inputs,c_inputs],outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "plot_model(model,to_file='model3.png',show_shapes=True,show_layer_names=True,rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 6.766035974025726\n",
      "Epoch : 2 Loss : 6.707617878913879\n",
      "Epoch : 3 Loss : 6.636330723762512\n",
      "Epoch : 4 Loss : 6.550922453403473\n",
      "Epoch : 5 Loss : 6.450494885444641\n"
     ]
    }
   ],
   "source": [
    "#모델의 학습은 5에포크 수행하겠다.\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "    print('Epoch :',epoch, 'Loss :',loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('commited', 0.41037195920944214),\n",
       " ('buddist', 0.3853633403778076),\n",
       " ('jails', 0.3764612674713135),\n",
       " ('binah', 0.3735484778881073),\n",
       " ('onatal', 0.3721325397491455),\n",
       " ('oooooo', 0.36630985140800476),\n",
       " ('falls', 0.3662239909172058),\n",
       " ('undercooking', 0.36434799432754517),\n",
       " ('extermination', 0.3570270240306854),\n",
       " ('benefitting', 0.3548571467399597)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=open('vectors.txt','w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1,embed_size))\n",
    "vectors=model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str,list(vectors[i,:])))))\n",
    "f.close()\n",
    "\n",
    "w2v=gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt',binary=False)\n",
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
