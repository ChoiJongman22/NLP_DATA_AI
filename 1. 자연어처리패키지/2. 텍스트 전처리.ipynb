{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "#코퍼스:언어 연구를 위해 텍스트를 컴퓨터가 읽을 수 있는 형태로 모아 놓은 언어 자료\n",
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "#텐서플로우에서 사용하면 아포스트로비는 보존한다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "#구두점을 별도로 분류하는 특징을 갖는다.\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "#표준 토큰화 예제\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "text=\"Starting a home-based restaurant may be an ideal.it doesn't have a food chain or restaurant of their own. \"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#문장 토큰화\n",
    "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "#한국어 자연어 처리방법\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "#정제와 정규화\n",
    "#길이가 1~2인 단어들을 정규 표현식을 이용해서 삭제\n",
    "import re\n",
    "text =\"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword=re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('',text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "#어간 추출 & 표제어 추출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "n=WordNetLemmatizer()\n",
    "words=['policy','doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([n.lemmatize(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('has','v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "#어간추출:\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "s = PorterStemmer() #토큰화\n",
    "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "words=word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "print([s.stem(w) for w in words])\n",
    "#이 알고리즘 결과에는 사전에 없는 단어들도 포함되어 있다. \n",
    "#이는 단순규칙에 의해 이루어지기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['formal', 'allow', 'electric']\n"
     ]
    }
   ],
   "source": [
    "words=['formalize', 'allowance', 'electricical']\n",
    "print([s.stem(w) for w in words])\n",
    "#alize->al\n",
    "#ance->제거\n",
    "#ical->ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "s=PorterStemmer()\n",
    "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([s.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#불용어 확인(영어중에 10개만)\n",
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['is', 'not', 'an']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example)\n",
    "result=[]\n",
    "for w in word_tokens:\n",
    "    if w in stop_words:\n",
    "        result.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
      "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
    "\n",
    "stop_words=stop_words.split(' ')\n",
    "word_tokens=word_tokenize(example)\n",
    "result=[]\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        result.append(w)\n",
    "#result=[word for word in word_tokens if not word in stop_words]\n",
    "print(word_tokens)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규 표현식\n",
    "import re\n",
    "r=re.compile(\"a.c\")\n",
    "r.search(\"kkk\")#아무런 결과도 출력되지 않는다. \n",
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"ab?c\")\n",
    "r.search(\"abbc\")\n",
    "#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ac'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*기호 \n",
    "import re\n",
    "r=re.compile(\"ab*c\")\n",
    "r.search(\"a\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"ab+c\")\n",
    "r.search(\"ac\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 6), match='abbbbc'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"abbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"^a\")\n",
    "r.search(\"bbc\") #아무런 결과도 출력되지 않는다. \n",
    "r.search(\"ab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "r=re.compile(\"ab{2}c\")\n",
    "r.search(\"ac\")#아무런 결과도 출력되지 않는다. \n",
    "r.search(\"abc\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"abbc\")\n",
    "r.search(\"abbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='abbc'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=re.compile(\"ab{2,8}c\")\n",
    "r.search(\"ac\") #아무런 결과도 출력되지 않는다.\n",
    "r.search(\"ac\") #아무런 결과도 출력되지 않는다.\n",
    "r.search(\"abc\") #아무런 결과도 출력되지 않는다.\n",
    "r.search(\"abbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"abbbbbbbbbbbc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 4), match='aabc'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"a{2,}bc\")\n",
    "r.search(\"bc\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"aabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 9), match='aaaaaaabc'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaaabc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"[abc]\")\n",
    "r.search(\"zzz\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"aaaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='b'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"baac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='a'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"[a-z]\")\n",
    "r.search(\"AAA\")#아무런 결과도 출력되지 않는다.\n",
    "r.search(\"aBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.search(\"111\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='d'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"[^abc]\")\n",
    "r.search(\"a\")\n",
    "r.search(\"ab\")\n",
    "r.search(\"b\")\n",
    "r.search(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 1), match='1'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.search(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r=re.compile(\"ab.\")\n",
    "r.search(\"kkkabc\")\n",
    "r.match(\"kkkabc\")\n",
    "r.match(\"abckkk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사과', '딸기', '수박', '메론', '바나나']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['010', '1234', '1234', '30']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"이름 : 김철수 전화번호 : 010 - 1234 -1234 나이 : 30 성별 : 남\"\"\"\n",
    "re.findall(\"\\d+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"\\d+\",\"문자열입니다.\") #빈 리스트를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expression   A regular expression  regex or regexp[ ]  sometimes called a rational expression [ ][ ] is  in theoretical computer science and formal language theory  a sequence of characters that define a search pattern '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "re.sub('[^a-zA-z]',' ',text)\n",
    "#정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', 'John', 'PROF', '101', 'James', 'STUD', '102', 'Mac', 'STUD']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text=\"\"\"100 John PROF\n",
    "101 James STUD\n",
    "102 Mac STUD\"\"\"\n",
    "re.split(\"\\s+\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J', 'P', 'R', 'O', 'F', 'J', 'S', 'T', 'U', 'D', 'M', 'S', 'T', 'U', 'D']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PROF', 'STUD', 'STUD']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z]{4}',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'James', 'Mac']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('[A-Z][a-z]+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "letters_only=re.sub('[^a-zA-Z]',' ',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\w]+\")\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer=RegexpTokenizer(\"[\\s]+\",gaps=True)\n",
    "print(tokenizer.tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "#정수 인코딩\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "text=sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab ={}#파이썬의 dictionary 자료형\n",
    "sentences=[]\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence=word_tokenize(i)\n",
    "    result=[]\n",
    "    \n",
    "    for word in sentence:\n",
    "        word=word.lower()\n",
    "        if word not in stop_words:\n",
    "            if len(word)>2:\n",
    "                result.append(word)\n",
    "                if word not in vocab:\n",
    "                    vocab[word]=0\n",
    "                vocab[word]+=1\n",
    "    sentences.append(result)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"]) #barber라는 단어의 빈도수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted=sorted(vocab.items(),key=lambda x:x[1],reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={}\n",
    "i=0\n",
    "for (word,frequency) in vocab_sorted:\n",
    "    if frequency>1:\n",
    "        i=i+1\n",
    "        word_to_index[word]=i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size=5\n",
    "words_frequency=[w for w,c in word_to_index.items() if c>=vocab_size+1]\n",
    "#인덱스가 5 초과인 단어 제거\n",
    "\n",
    "for w in words_frequency:\n",
    "    del word_to_index[w]#해당 단어에 다한 인덱스 정보를 삭제\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 6, 5], [1, 3, 5], [6, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [6, 6, 3, 2, 6, 1, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "word_to_index['OOV']=len(word_to_index)+1\n",
    "encoded=[]\n",
    "for s in sentences:\n",
    "    temp=[]\n",
    "    for w in s:\n",
    "        try:\n",
    "            temp.append(word_to_index[w])\n",
    "        except KeyError:\n",
    "            temp.append(word_to_index['OOV'])\n",
    "    encoded.append(temp)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "words=sum(sentences,[])\n",
    "#문장의 경계를 제거하고 단어들을 하나의 리스트로 만든다.\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "vocab=Counter(words)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=5\n",
    "vocab=vocab.most_common(vocab_size)#등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={}\n",
    "i=0\n",
    "for (word,frequency) in vocab:\n",
    "    i=i+1\n",
    "    word_to_index[word]=i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "vocab=FreqDist(np.hstack(sentences))\n",
    "print(vocab[\"barber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=5\n",
    "vocab=vocab.most_common(vocab_size)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index={word[0]:index+1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value:a,index:0\n",
      "value:b,index:1\n",
      "value:c,index:2\n",
      "value:d,index:3\n",
      "value:e,index:4\n"
     ]
    }
   ],
   "source": [
    "test=['a','b','c','d','e']\n",
    "for index, value in enumerate(test):\n",
    "    print(\"value:{},index:{}\".format(value,index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "#토큰화까지 수행된 앞서 사용한 텍스트 데이터와 동일한 데이터를 사용합니다.\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)#코퍼스 입력으로 하면 빈도수를 기준으로 단어집합을 생성한다.\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "vocab_size=5\n",
    "tokenizer=Tokenizer(num_words=vocab_size+1)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 OOV의 인덱스:1\n"
     ]
    }
   ],
   "source": [
    "vocab_size=5\n",
    "tokenizer=Tokenizer(num_words=vocab_size+2,oov_token='OOV')\n",
    "#빈도수 상위 5개 단어만 사용. 숫자 0과 OOV를 고려해서 단어 집합의 크기는 +2\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print('단어 OOV의 인덱스:{}'.format(tokenizer.word_index['OOV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 6], [2, 1, 6], [2, 4, 6], [1, 3], [3, 5, 4, 3], [4, 3], [2, 5, 1], [2, 5, 1], [2, 5, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences) #fit_on_texts() 안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
    "encoded=tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len=max(len(item) for item in encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in encoded:\n",
    "    while len(item) < max_len:\n",
    "        item.append(0)\n",
    "padded_np=np.array(encoded)\n",
    "padded_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoded=tokenizer.texts_to_sequences(sentences)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded=pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded=pad_sequences(encoded,padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(padded==padded_np).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  0],\n",
       "       [ 3,  2, 10,  1, 11],\n",
       "       [ 3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded=pad_sequences(encoded,padding='post',maxlen=5)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_value=len(tokenizer.word_index)+1\n",
    "print(last_value)\n",
    "padded =pad_sequences(encoded, padding='post',value=last_value)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다', '.']\n"
     ]
    }
   ],
   "source": [
    "#원 핫 코딩\n",
    "from konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "token=okt.morphs(\"나는 자연어 처리를 배운다.\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5, '.': 6}\n"
     ]
    }
   ],
   "source": [
    "word2index={}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word2index):\n",
    "    one_hot_vector=[0]*(len(word2index))\n",
    "    index=word2index[word]\n",
    "    one_hot_vector[index]=1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\",word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "#keras를 이용한 원 핫 인코딩 \n",
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "t=Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 1, 6, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded=t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot=to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x230.4 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60/60 [==============================] - 1s 14ms/step - loss: 1.4570 - accuracy: 0.5843 - val_loss: 0.8142 - val_accuracy: 0.8179\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6330 - accuracy: 0.8440 - val_loss: 0.4940 - val_accuracy: 0.8762\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.4537 - accuracy: 0.8795 - val_loss: 0.3950 - val_accuracy: 0.8944\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.3813 - accuracy: 0.8959 - val_loss: 0.3453 - val_accuracy: 0.9054\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.3393 - accuracy: 0.9061 - val_loss: 0.3140 - val_accuracy: 0.9114\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.3110 - accuracy: 0.9132 - val_loss: 0.2912 - val_accuracy: 0.9158\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.2905 - accuracy: 0.9197 - val_loss: 0.2759 - val_accuracy: 0.9202\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.92 - 0s 6ms/step - loss: 0.2746 - accuracy: 0.9237 - val_loss: 0.2641 - val_accuracy: 0.9228\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.2622 - accuracy: 0.9271 - val_loss: 0.2539 - val_accuracy: 0.9257\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.2524 - accuracy: 0.9295 - val_loss: 0.2455 - val_accuracy: 0.9278\n",
      "Test loss: 0.24546048045158386\n",
      "Test accuracy 0.9277999997138977\n",
      "Computation time:[0:.3f]sec\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
